{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stealing stuff... http://brandonrose.org/clustering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, re, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import *\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from datetime import datetime, timedelta, date\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import defaultdict\n",
    "from data_science_toolkit.dataset_ops import classifier_train_val_test_dfs\n",
    "from data_science_toolkit.data_visualization import get_fig_ax, visualize_class_distribution, top_n_tokens_plot_from_counter\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include a paragraph in your final report about why you changed your topic\n",
    "- Predict more or less than avg number of retweets for that year for each tweet\n",
    "- LDA for topic modelling (investigate)\n",
    "- Apriori for combos of words\n",
    "- Binary classifier\n",
    "- In general we use clustering or topic modelling to help understand the data, but we don't usually use it for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = 'above_monthly_avg'\n",
    "rand_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_tweet = pd.read_csv('./since_election_with_cluster_trump_tweets_sp500.csv')\n",
    "stock_tweet['created_at']= pd.to_datetime(stock_tweet['created_at']) \n",
    "stock_tweet['dow'] = stock_tweet.dow.astype('category')\n",
    "stock_tweet['num_links'] = stock_tweet.num_links.astype('category')\n",
    "stock_tweet['created_hour'] = stock_tweet.created_hour.astype('category')\n",
    "# These would be better as categorical variables, but there are not enough of them for k fold to work properly?\n",
    "# stock_tweet['num_mentions'] = stock_tweet.num_mentions.astype('category')\n",
    "# stock_tweet['num_hashtags'] = stock_tweet.num_hashtags.astype('category')\n",
    "# stock_tweet['percent_caps'] = stock_tweet.percent_caps.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [x for x in stock_tweet.columns.tolist() if '_apr_' in x]:\n",
    "    stock_tweet[col] = stock_tweet[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')).union({''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens_without_letters(text):\n",
    "    tokens = text.split(\" \")\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "def split_text_only(text):\n",
    "    return text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.field]\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[[self.field]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(numeric_features, categorical_features):\n",
    "    # We create the preprocessing pipelines for both numeric and categorical data.\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='error', categories='auto'))])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "    preprocessing_pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('text', Pipeline([\n",
    "                ('colext', TextSelector('preprocessed_text')),\n",
    "                ('tfidf', TfidfVectorizer(max_df=0.9,min_df=3,use_idf=True, tokenizer=filter_tokens_without_letters, ngram_range=(1,4)))\n",
    "            ])),\n",
    "            ('bow', Pipeline([\n",
    "                ('colext', TextSelector('preprocessed_text')),\n",
    "                ('bow', CountVectorizer(max_df=0.5, min_df=5, tokenizer=split_text_only, ngram_range=(1, 5)))\n",
    "            ])),\n",
    "        ]))\n",
    "    ])\n",
    "#     svm_linear = Pipeline(steps=[('yeet', preprocessing_pipeline),\n",
    "#                           ('classifier', SVC(probability=True, gamma='scale', kernel='linear'))])\n",
    "    svm_rbf = Pipeline(steps=[('yeet', preprocessing_pipeline),\n",
    "                          ('classifier', SVC(probability=True, gamma='scale', kernel='rbf'))])\n",
    "    random_forest = Pipeline(steps=[('yeet', preprocessing_pipeline),\n",
    "                            ('classifier', RandomForestClassifier(n_estimators=100, random_state=rand_state))])\n",
    "    return svm_rbf, random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.6987138263665594], 'roc': [0.7560482969043508]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.6787781350482315], 'roc': [0.7375334701456074]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.6938906752411576], 'roc': [0.758147605242299]})\n",
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.6987138263665594, 0.7237942122186495], 'roc': [0.7560482969043508, 0.7833792266708868]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.6787781350482315, 0.684887459807074], 'roc': [0.7375334701456074, 0.7554391517645197]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.6938906752411576, 0.7160771704180064], 'roc': [0.758147605242299, 0.7819186580155087]})\n",
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.6987138263665594, 0.7237942122186495, 0.6876809263428755], 'roc': [0.7560482969043508, 0.7833792266708868, 0.7310420222751447]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.6787781350482315, 0.684887459807074, 0.6516564811836604], 'roc': [0.7375334701456074, 0.7554391517645197, 0.6888732449550443]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.6938906752411576, 0.7160771704180064, 0.6789964618848504], 'roc': [0.758147605242299, 0.7819186580155087, 0.7205251677109206]})\n"
     ]
    }
   ],
   "source": [
    "model_1 = defaultdict(list)\n",
    "model_2 = defaultdict(list)\n",
    "model_avg = defaultdict(list)\n",
    "kf = KFold(n_splits=3, random_state=rand_state)\n",
    "for train_index, test_index in kf.split(stock_tweet):\n",
    "    print(\"New Fold...\")\n",
    "    curr_train_df = stock_tweet.iloc[train_index]\n",
    "    curr_train_out = curr_train_df[output_col].tolist()\n",
    "    curr_test_df = stock_tweet.iloc[test_index]\n",
    "    curr_test_out = curr_test_df[output_col].tolist()\n",
    "    numeric_features = ['num_words', 'percent_caps', 'num_mentions', 'num_hashtags']\n",
    "    categorical_features = ['created_hour', 'dow', 'num_links'] + [x for x in stock_tweet.columns.tolist() if '_apr_' in x]\n",
    "    svm_rbf, random_forest = create_models(numeric_features, categorical_features)\n",
    "    random_forest.fit(curr_train_df, curr_train_out)\n",
    "    rf_out = random_forest.predict_proba(curr_test_df)\n",
    "    model_2['roc'].append(roc_auc_score(curr_test_out, rf_out[:,1]))\n",
    "    model_2['acc'].append(accuracy_score(curr_test_out, np.round(rf_out[:,1])))\n",
    "    svm_rbf.fit(curr_train_df, curr_train_out)\n",
    "    svm_rbf_out = svm_rbf.predict_proba(curr_test_df)\n",
    "    model_1['roc'].append(roc_auc_score(curr_test_out, svm_rbf_out[:,1]))\n",
    "    model_1['acc'].append(accuracy_score(curr_test_out, np.round(svm_rbf_out[:,1])))\n",
    "    averaged_preds = np.mean(np.array([rf_out[:,1], svm_rbf_out[:,1]]), axis=0)\n",
    "    model_avg['roc'].append(roc_auc_score(curr_test_out, averaged_preds))\n",
    "    model_avg['acc'].append(accuracy_score(curr_test_out, np.round(averaged_preds)))\n",
    "    print(model_1)\n",
    "    print(model_2)\n",
    "    print(model_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.7033963216426948, ROC: 0.7568231819501275\n",
      "Acc: 0.671774025346322, ROC: 0.7272819556217239\n",
      "Acc: 0.6963214358480049, ROC: 0.753530476989576\n"
     ]
    }
   ],
   "source": [
    "for model in [model_1, model_2, model_avg]:\n",
    "    print(\"Acc: {}, ROC: {}\".format(np.mean(model['acc']), np.mean(model['roc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
