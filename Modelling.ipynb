{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, re, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import *\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from datetime import datetime, timedelta, date\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import defaultdict\n",
    "from data_science_toolkit.dataset_ops import classifier_train_val_test_dfs\n",
    "from data_science_toolkit.data_visualization import get_fig_ax, visualize_class_distribution, top_n_tokens_plot_from_counter\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include a paragraph in your final report about why you changed your topic\n",
    "- Predict more or less than avg number of retweets for that year for each tweet\n",
    "- LDA for topic modelling (investigate)\n",
    "- Apriori for combos of words\n",
    "- Binary classifier\n",
    "- In general we use clustering or topic modelling to help understand the data, but we don't usually use it for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'since_election_with_cluster_elonmusk_twint_preprocessed.csv'\n",
    "# file_name = \"since_election_with_cluster_trump_tweets_sp500.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = 'above_monthly_avg'\n",
    "rand_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_tweet = pd.read_csv(\"./{}\".format(file_name))\n",
    "stock_tweet['created_at']= pd.to_datetime(stock_tweet['created_at']) \n",
    "stock_tweet['dow'] = stock_tweet.dow.astype('category')\n",
    "stock_tweet['num_links'] = stock_tweet.num_links.astype('category')\n",
    "stock_tweet['created_hour'] = stock_tweet.created_hour.astype('category')\n",
    "# These would be better as categorical variables, but there are not enough of them for k fold to work properly?\n",
    "# stock_tweet['num_mentions'] = stock_tweet.num_mentions.astype('category')\n",
    "# stock_tweet['num_hashtags'] = stock_tweet.num_hashtags.astype('category')\n",
    "# stock_tweet['percent_caps'] = stock_tweet.percent_caps.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in [x for x in stock_tweet.columns.tolist() if '_apr_' in x]:\n",
    "#     stock_tweet[col] = stock_tweet[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')).union({''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens_without_letters(text):\n",
    "    tokens = text.split(\" \")\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "def split_text_only(text):\n",
    "    return text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.field]\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[[self.field]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(numeric_features, categorical_features):\n",
    "    # We create the preprocessing pipelines for both numeric and categorical data.\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='error', categories='auto'))])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "    preprocessing_pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('text', Pipeline([\n",
    "                ('colext', TextSelector('preprocessed_text')),\n",
    "                ('tfidf', TfidfVectorizer(max_df=0.9,min_df=3,use_idf=True, tokenizer=filter_tokens_without_letters, ngram_range=(1,4)))\n",
    "            ])),\n",
    "            ('bow', Pipeline([\n",
    "                ('colext', TextSelector('preprocessed_text')),\n",
    "                ('bow', CountVectorizer(max_df=0.5, min_df=5, tokenizer=split_text_only, ngram_range=(1, 5)))\n",
    "            ])),\n",
    "        ]))\n",
    "    ])\n",
    "#     svm_linear = Pipeline(steps=[('yeet', preprocessing_pipeline),\n",
    "#                           ('classifier', SVC(probability=True, gamma='scale', kernel='linear'))])\n",
    "    svm_rbf = Pipeline(steps=[('yeet', preprocessing_pipeline),\n",
    "                          ('classifier', SVC(probability=True, gamma='scale', kernel='rbf'))])\n",
    "    random_forest = Pipeline(steps=[('yeet', preprocessing_pipeline),\n",
    "                            ('classifier', RandomForestClassifier(n_estimators=100, random_state=rand_state))])\n",
    "    log_reg = Pipeline(steps=[('yeet', preprocessing_pipeline),\n",
    "                            ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "    return svm_rbf, random_forest, log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Fold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM RBF\n",
      "defaultdict(<class 'list'>, {'acc': [0.8180936995153474], 'roc': [0.7399191220112207], 'f1': [0.18287373004354135]})\n",
      "SVM RBF with extra features\n",
      "defaultdict(<class 'list'>, {'acc': [0.8180936995153474], 'roc': [0.7378746800733715], 'f1': [0.19225251076040173]})\n",
      "Random Forest\n",
      "defaultdict(<class 'list'>, {'acc': [0.8193861066235865], 'roc': [0.7070455699058147], 'f1': [0.23319615912208505]})\n",
      "Random Forest with extra features\n",
      "defaultdict(<class 'list'>, {'acc': [0.8206785137318255], 'roc': [0.7266225282059132], 'f1': [0.24076607387140905]})\n",
      "Logistic Regression\n",
      "defaultdict(<class 'list'>, {'acc': [0.8164781906300485], 'roc': [0.6929285136873542], 'f1': [0.32057416267942584]})\n",
      "Logistic Regression with extra features\n",
      "defaultdict(<class 'list'>, {'acc': [0.8148626817447496], 'roc': [0.7068240047696931], 'f1': [0.32189349112426036]})\n",
      "New Fold...\n"
     ]
    }
   ],
   "source": [
    "svm_rbf_results = defaultdict(list)\n",
    "random_forest_results = defaultdict(list)\n",
    "log_reg_results = defaultdict(list)\n",
    "svm_rbf_with_extra_features_results = defaultdict(list)\n",
    "random_forest_with_extra_features_results = defaultdict(list)\n",
    "log_reg_results = defaultdict(list)\n",
    "log_reg_with_extra_features_results = defaultdict(list)\n",
    "# model_avg = defaultdict(list)\n",
    "kf = KFold(n_splits=2, random_state=rand_state)\n",
    "for train_index, test_index in kf.split(stock_tweet):\n",
    "    print(\"New Fold...\")\n",
    "    curr_train_df = stock_tweet.iloc[train_index]\n",
    "    curr_train_out = curr_train_df[output_col].tolist()\n",
    "    curr_test_df = stock_tweet.iloc[test_index]\n",
    "    curr_test_out = curr_test_df[output_col].tolist()\n",
    "    numeric_features = ['num_words', 'percent_caps', 'num_mentions', 'num_hashtags', \"num_photos\", \"num_videos\"]\n",
    "    categorical_features = ['created_hour', 'dow', 'num_links']# + [x for x in stock_tweet.columns.tolist() if '_apr_' in x]\n",
    "    svm_rbf, random_forest, log_reg = create_models([], [])\n",
    "    svm_rbf_with_extra_features, random_forest_with_extra_features, log_reg_with_extra_features = create_models(numeric_features, categorical_features)\n",
    "    # Random Forest\n",
    "    random_forest.fit(curr_train_df, curr_train_out)\n",
    "    rf_out = random_forest.predict_proba(curr_test_df)\n",
    "    random_forest_results['roc'].append(roc_auc_score(curr_test_out, rf_out[:,1]))\n",
    "    random_forest_results['acc'].append(accuracy_score(curr_test_out, np.round(rf_out[:,1])))\n",
    "    random_forest_results['f1'].append(f1_score(curr_test_out, np.round(rf_out[:,1])))\n",
    "    # SVM RBF\n",
    "    svm_rbf.fit(curr_train_df, curr_train_out)\n",
    "    svm_rbf_out = svm_rbf.predict_proba(curr_test_df)\n",
    "    svm_rbf_results['roc'].append(roc_auc_score(curr_test_out, svm_rbf_out[:,1]))\n",
    "    svm_rbf_results['acc'].append(accuracy_score(curr_test_out, np.round(svm_rbf_out[:,1])))\n",
    "    svm_rbf_results['f1'].append(f1_score(curr_test_out, np.round(svm_rbf_out[:,1])))\n",
    "    # Log Reg\n",
    "    log_reg.fit(curr_train_df, curr_train_out)\n",
    "    log_reg_out = log_reg.predict_proba(curr_test_df)\n",
    "    log_reg_results['roc'].append(roc_auc_score(curr_test_out, log_reg_out[:,1]))\n",
    "    log_reg_results['acc'].append(accuracy_score(curr_test_out, np.round(log_reg_out[:,1])))\n",
    "    log_reg_results['f1'].append(f1_score(curr_test_out, np.round(log_reg_out[:,1])))\n",
    "    # Random Forest with Extra Features:\n",
    "    random_forest_with_extra_features.fit(curr_train_df, curr_train_out)\n",
    "    rf_out = random_forest_with_extra_features.predict_proba(curr_test_df)\n",
    "    random_forest_with_extra_features_results['roc'].append(roc_auc_score(curr_test_out, rf_out[:,1]))\n",
    "    random_forest_with_extra_features_results['acc'].append(accuracy_score(curr_test_out, np.round(rf_out[:,1])))\n",
    "    random_forest_with_extra_features_results['f1'].append(f1_score(curr_test_out, np.round(rf_out[:,1])))\n",
    "    # SVM RBF with Extra Features\n",
    "    svm_rbf_with_extra_features.fit(curr_train_df, curr_train_out)\n",
    "    svm_rbf_with_extra_features_out = svm_rbf_with_extra_features.predict_proba(curr_test_df)\n",
    "    svm_rbf_with_extra_features_results['roc'].append(roc_auc_score(curr_test_out, svm_rbf_with_extra_features_out[:,1]))\n",
    "    svm_rbf_with_extra_features_results['acc'].append(accuracy_score(curr_test_out, np.round(svm_rbf_with_extra_features_out[:,1])))\n",
    "    svm_rbf_with_extra_features_results['f1'].append(f1_score(curr_test_out, np.round(svm_rbf_with_extra_features_out[:,1])))\n",
    "    # Log Reg\n",
    "    log_reg_with_extra_features.fit(curr_train_df, curr_train_out)\n",
    "    log_reg_with_extra_features_results_out = log_reg_with_extra_features.predict_proba(curr_test_df)\n",
    "    log_reg_with_extra_features_results['roc'].append(roc_auc_score(curr_test_out, log_reg_with_extra_features_results_out[:,1]))\n",
    "    log_reg_with_extra_features_results['acc'].append(accuracy_score(curr_test_out, np.round(log_reg_with_extra_features_results_out[:,1])))\n",
    "    log_reg_with_extra_features_results['f1'].append(f1_score(curr_test_out, np.round(log_reg_with_extra_features_results_out[:,1])))\n",
    "    \n",
    "#     averaged_preds = np.mean(np.array([rf_out[:,1], svm_rbf_out[:,1]]), axis=0)\n",
    "#     model_avg['roc'].append(roc_auc_score(curr_test_out, averaged_preds))\n",
    "#     model_avg['acc'].append(accuracy_score(curr_test_out, np.round(averaged_preds)))\n",
    "    print(\"SVM RBF\")\n",
    "    print(svm_rbf_results)\n",
    "    print(\"SVM RBF with extra features\")\n",
    "    print(svm_rbf_with_extra_features_results)\n",
    "    print(\"Random Forest\")\n",
    "    print(random_forest_results)\n",
    "    print(\"Random Forest with extra features\")\n",
    "    print(random_forest_with_extra_features_results)\n",
    "    print(\"Logistic Regression\")\n",
    "    print(log_reg_results)\n",
    "    print(\"Logistic Regression with extra features\")\n",
    "    print(log_reg_with_extra_features_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.851373182552504, ROC: 0.7597295776506516\n",
      "Acc: 0.8466882067851373, ROC: 0.7482297929084507\n",
      "Acc: 0.8389337641357028, ROC: 0.7495433437255496\n",
      "Acc: 0.8502423263327948, ROC: 0.7600222903636887\n",
      "Acc: 0.8491114701130856, ROC: 0.7574460987665825\n",
      "Acc: 0.8350565428109855, ROC: 0.7555562392031534\n"
     ]
    }
   ],
   "source": [
    "for model in [svm_rbf_results, random_forest_results, log_reg_results, svm_rbf_with_extra_features_results, random_forest_with_extra_features_results, log_reg_with_extra_features_results]:\n",
    "    print(\"Acc: {}, ROC: {}\".format(np.mean(model['acc']), \n",
    "                                    np.mean(model['roc']),\n",
    "                                    np.mean(model['f1'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
