{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stealing stuff... http://brandonrose.org/clustering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, re, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import *\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from datetime import datetime, timedelta, date\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import defaultdict\n",
    "from data_science_toolkit.dataset_ops import classifier_train_val_test_dfs\n",
    "from data_science_toolkit.data_visualization import get_fig_ax, visualize_class_distribution, top_n_tokens_plot_from_counter\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include a paragraph in your final report about why you changed your topic\n",
    "- Predict more or less than avg number of retweets for that year for each tweet\n",
    "- LDA for topic modelling (investigate)\n",
    "- Apriori for combos of words\n",
    "- Binary classifier\n",
    "- In general we use clustering or topic modelling to help understand the data, but we don't usually use it for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'since_election_with_cluster_elonmusk_twint_preprocessed.csv'\n",
    "# file_name = \"since_election_with_cluster_trump_tweets_sp500.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = 'above_monthly_avg'\n",
    "rand_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_tweet = pd.read_csv(\"./{}\".format(file_name))\n",
    "stock_tweet['created_at']= pd.to_datetime(stock_tweet['created_at']) \n",
    "stock_tweet['dow'] = stock_tweet.dow.astype('category')\n",
    "stock_tweet['num_links'] = stock_tweet.num_links.astype('category')\n",
    "stock_tweet['created_hour'] = stock_tweet.created_hour.astype('category')\n",
    "# These would be better as categorical variables, but there are not enough of them for k fold to work properly?\n",
    "# stock_tweet['num_mentions'] = stock_tweet.num_mentions.astype('category')\n",
    "# stock_tweet['num_hashtags'] = stock_tweet.num_hashtags.astype('category')\n",
    "# stock_tweet['percent_caps'] = stock_tweet.percent_caps.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in [x for x in stock_tweet.columns.tolist() if '_apr_' in x]:\n",
    "#     stock_tweet[col] = stock_tweet[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')).union({''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens_without_letters(text):\n",
    "    tokens = text.split(\" \")\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "def split_text_only(text):\n",
    "    return text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.field]\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[[self.field]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(numeric_features, categorical_features):\n",
    "    # We create the preprocessing pipelines for both numeric and categorical data.\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='error', categories='auto'))])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "    preprocessing_pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('text', Pipeline([\n",
    "                ('colext', TextSelector('preprocessed_text')),\n",
    "                ('tfidf', TfidfVectorizer(max_df=0.9,min_df=3,use_idf=True, tokenizer=filter_tokens_without_letters, ngram_range=(1,4)))\n",
    "            ])),\n",
    "            ('bow', Pipeline([\n",
    "                ('colext', TextSelector('preprocessed_text')),\n",
    "                ('bow', CountVectorizer(max_df=0.5, min_df=5, tokenizer=split_text_only, ngram_range=(1, 5)))\n",
    "            ])),\n",
    "        ]))\n",
    "    ])\n",
    "#     svm_linear = Pipeline(steps=[('yeet', preprocessing_pipeline),\n",
    "#                           ('classifier', SVC(probability=True, gamma='scale', kernel='linear'))])\n",
    "    svm_rbf = Pipeline(steps=[('yeet', preprocessing_pipeline),\n",
    "                          ('classifier', SVC(probability=True, gamma='scale', kernel='rbf'))])\n",
    "    random_forest = Pipeline(steps=[('yeet', preprocessing_pipeline),\n",
    "                            ('classifier', RandomForestClassifier(n_estimators=100, random_state=rand_state))])\n",
    "    return svm_rbf, random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929], 'roc': [0.8074331983805667]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.8126009693053312], 'roc': [0.8129554655870445]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929], 'roc': [0.8251012145748988]})\n",
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493], 'roc': [0.8074331983805667, 0.7689663277898572]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.8126009693053312, 0.7980613893376414], 'roc': [0.8129554655870445, 0.7848621966269025]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493], 'roc': [0.8251012145748988, 0.7907974378562614]})\n",
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.7980613893376414], 'roc': [0.8074331983805667, 0.7689663277898572, 0.7146289460285132]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.8126009693053312, 0.7980613893376414, 0.7980613893376414], 'roc': [0.8129554655870445, 0.7848621966269025, 0.6808569882892056]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.802907915993538], 'roc': [0.8251012145748988, 0.7907974378562614, 0.6967286150712831]})\n",
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.7980613893376414, 0.840064620355412], 'roc': [0.8074331983805667, 0.7689663277898572, 0.7146289460285132, 0.7347662372414847]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.8126009693053312, 0.7980613893376414, 0.7980613893376414, 0.8465266558966075], 'roc': [0.8129554655870445, 0.7848621966269025, 0.6808569882892056, 0.7492258878397493]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.802907915993538, 0.840064620355412], 'roc': [0.8251012145748988, 0.7907974378562614, 0.6967286150712831, 0.7581616269735082]})\n",
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.7980613893376414, 0.840064620355412, 0.851373182552504], 'roc': [0.8074331983805667, 0.7689663277898572, 0.7146289460285132, 0.7347662372414847, 0.7542346542346543]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.8126009693053312, 0.7980613893376414, 0.7980613893376414, 0.8465266558966075, 0.8610662358642972], 'roc': [0.8129554655870445, 0.7848621966269025, 0.6808569882892056, 0.7492258878397493, 0.7476787101787101]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.802907915993538, 0.840064620355412, 0.8529886914378029], 'roc': [0.8251012145748988, 0.7907974378562614, 0.6967286150712831, 0.7581616269735082, 0.7666083916083916]})\n",
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.7980613893376414, 0.840064620355412, 0.851373182552504, 0.8594507269789984], 'roc': [0.8074331983805667, 0.7689663277898572, 0.7146289460285132, 0.7347662372414847, 0.7542346542346543, 0.7924759053738317]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.8126009693053312, 0.7980613893376414, 0.7980613893376414, 0.8465266558966075, 0.8610662358642972, 0.851373182552504], 'roc': [0.8129554655870445, 0.7848621966269025, 0.6808569882892056, 0.7492258878397493, 0.7476787101787101, 0.8104829877336449]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.802907915993538, 0.840064620355412, 0.8529886914378029, 0.8546042003231018], 'roc': [0.8251012145748988, 0.7907974378562614, 0.6967286150712831, 0.7581616269735082, 0.7666083916083916, 0.8173919392523366]})\n",
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.7980613893376414, 0.840064620355412, 0.851373182552504, 0.8594507269789984, 0.8917609046849758], 'roc': [0.8074331983805667, 0.7689663277898572, 0.7146289460285132, 0.7347662372414847, 0.7542346542346543, 0.7924759053738317, 0.797196261682243]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.8126009693053312, 0.7980613893376414, 0.7980613893376414, 0.8465266558966075, 0.8610662358642972, 0.851373182552504, 0.8852988691437803], 'roc': [0.8129554655870445, 0.7848621966269025, 0.6808569882892056, 0.7492258878397493, 0.7476787101787101, 0.8104829877336449, 0.8387405429461504]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.802907915993538, 0.840064620355412, 0.8529886914378029, 0.8546042003231018, 0.8949919224555735], 'roc': [0.8251012145748988, 0.7907974378562614, 0.6967286150712831, 0.7581616269735082, 0.7666083916083916, 0.8173919392523366, 0.8423676012461059]})\n",
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.7980613893376414, 0.840064620355412, 0.851373182552504, 0.8594507269789984, 0.8917609046849758, 0.8562197092084006], 'roc': [0.8074331983805667, 0.7689663277898572, 0.7146289460285132, 0.7347662372414847, 0.7542346542346543, 0.7924759053738317, 0.797196261682243, 0.7546637704439252]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.8126009693053312, 0.7980613893376414, 0.7980613893376414, 0.8465266558966075, 0.8610662358642972, 0.851373182552504, 0.8852988691437803, 0.8529886914378029], 'roc': [0.8129554655870445, 0.7848621966269025, 0.6808569882892056, 0.7492258878397493, 0.7476787101787101, 0.8104829877336449, 0.8387405429461504, 0.782573379088785]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.802907915993538, 0.840064620355412, 0.8529886914378029, 0.8546042003231018, 0.8949919224555735, 0.8594507269789984], 'roc': [0.8251012145748988, 0.7907974378562614, 0.6967286150712831, 0.7581616269735082, 0.7666083916083916, 0.8173919392523366, 0.8423676012461059, 0.7839880257009346]})\n",
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.7980613893376414, 0.840064620355412, 0.851373182552504, 0.8594507269789984, 0.8917609046849758, 0.8562197092084006, 0.9176090468497576], 'roc': [0.8074331983805667, 0.7689663277898572, 0.7146289460285132, 0.7347662372414847, 0.7542346542346543, 0.7924759053738317, 0.797196261682243, 0.7546637704439252, 0.861770700033177]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.8126009693053312, 0.7980613893376414, 0.7980613893376414, 0.8465266558966075, 0.8610662358642972, 0.851373182552504, 0.8852988691437803, 0.8529886914378029, 0.9111470113085622], 'roc': [0.8129554655870445, 0.7848621966269025, 0.6808569882892056, 0.7492258878397493, 0.7476787101787101, 0.8104829877336449, 0.8387405429461504, 0.782573379088785, 0.868252049860183]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.802907915993538, 0.840064620355412, 0.8529886914378029, 0.8546042003231018, 0.8949919224555735, 0.8594507269789984, 0.9192245557350566], 'roc': [0.8251012145748988, 0.7907974378562614, 0.6967286150712831, 0.7581616269735082, 0.7666083916083916, 0.8173919392523366, 0.8423676012461059, 0.7839880257009346, 0.8749229821318546]})\n",
      "New Fold...\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.7980613893376414, 0.840064620355412, 0.851373182552504, 0.8594507269789984, 0.8917609046849758, 0.8562197092084006, 0.9176090468497576, 0.8852988691437803], 'roc': [0.8074331983805667, 0.7689663277898572, 0.7146289460285132, 0.7347662372414847, 0.7542346542346543, 0.7924759053738317, 0.797196261682243, 0.7546637704439252, 0.861770700033177, 0.768275980140387]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.8126009693053312, 0.7980613893376414, 0.7980613893376414, 0.8465266558966075, 0.8610662358642972, 0.851373182552504, 0.8852988691437803, 0.8529886914378029, 0.9111470113085622, 0.8804523424878837], 'roc': [0.8129554655870445, 0.7848621966269025, 0.6808569882892056, 0.7492258878397493, 0.7476787101787101, 0.8104829877336449, 0.8387405429461504, 0.782573379088785, 0.868252049860183, 0.7956257490155795]})\n",
      "defaultdict(<class 'list'>, {'acc': [0.815831987075929, 0.7867528271405493, 0.802907915993538, 0.840064620355412, 0.8529886914378029, 0.8546042003231018, 0.8949919224555735, 0.8594507269789984, 0.9192245557350566, 0.8852988691437803], 'roc': [0.8251012145748988, 0.7907974378562614, 0.6967286150712831, 0.7581616269735082, 0.7666083916083916, 0.8173919392523366, 0.8423676012461059, 0.7839880257009346, 0.8749229821318546, 0.7912172573189522]})\n"
     ]
    }
   ],
   "source": [
    "model_1 = defaultdict(list)\n",
    "model_2 = defaultdict(list)\n",
    "model_avg = defaultdict(list)\n",
    "kf = KFold(n_splits=10, random_state=rand_state)\n",
    "for train_index, test_index in kf.split(stock_tweet):\n",
    "    print(\"New Fold...\")\n",
    "    curr_train_df = stock_tweet.iloc[train_index]\n",
    "    curr_train_out = curr_train_df[output_col].tolist()\n",
    "    curr_test_df = stock_tweet.iloc[test_index]\n",
    "    curr_test_out = curr_test_df[output_col].tolist()\n",
    "    numeric_features = ['num_words', 'percent_caps', 'num_mentions', 'num_hashtags', \"num_photos\", \"num_videos\"]\n",
    "    categorical_features = ['created_hour', 'dow', 'num_links']# + [x for x in stock_tweet.columns.tolist() if '_apr_' in x]\n",
    "    svm_rbf, random_forest = create_models(numeric_features, categorical_features)\n",
    "    random_forest.fit(curr_train_df, curr_train_out)\n",
    "    rf_out = random_forest.predict_proba(curr_test_df)\n",
    "    model_2['roc'].append(roc_auc_score(curr_test_out, rf_out[:,1]))\n",
    "    model_2['acc'].append(accuracy_score(curr_test_out, np.round(rf_out[:,1])))\n",
    "    svm_rbf.fit(curr_train_df, curr_train_out)\n",
    "    svm_rbf_out = svm_rbf.predict_proba(curr_test_df)\n",
    "    model_1['roc'].append(roc_auc_score(curr_test_out, svm_rbf_out[:,1]))\n",
    "    model_1['acc'].append(accuracy_score(curr_test_out, np.round(svm_rbf_out[:,1])))\n",
    "    averaged_preds = np.mean(np.array([rf_out[:,1], svm_rbf_out[:,1]]), axis=0)\n",
    "    model_avg['roc'].append(roc_auc_score(curr_test_out, averaged_preds))\n",
    "    model_avg['acc'].append(accuracy_score(curr_test_out, np.round(averaged_preds)))\n",
    "    print(model_1)\n",
    "    print(model_2)\n",
    "    print(model_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8502423263327948, ROC: 0.775441198134864\n",
      "Acc: 0.8497576736672052, ROC: 0.7871253957165953\n",
      "Acc: 0.8512116316639743, ROC: 0.7947285091734527\n"
     ]
    }
   ],
   "source": [
    "for model in [model_1, model_2, model_avg]:\n",
    "    print(\"Acc: {}, ROC: {}\".format(np.mean(model['acc']), np.mean(model['roc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
